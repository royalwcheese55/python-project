1. volume/ the amount of data generated, the amount of user activity logs generated by amazon
velocity/ the speed of data generated and processed, real time stock price
variety/  different types and format of data, raw data and structured data table
veracity/ data quality and accuracy, is it trustworthy, email that could contain spams.

2. distributed storage: data is stored across multiple machines for fault tolerance and scalability.
hdfs store large files by splitting them into blocks across nodes.
distributed compute: computation is perform paralleled in multiple machines
spark process data across a cluster

3. hdfs splits files into fixed-size blocks and distribute them across multiple nodes.
namenode: stored metadata like file name and block locations/ permissions
datanode: stored actual data blocks
if one node fail the replication provide fault tolerance and data availability.

4. map: process input data and have key-value pairs
reduce: aggregate value for each key.
it is to solve massive dataset in parallel with fault tolerance.

5. spark support more workloads like streaming and machine learning, mapreduce mainly support batch
spark is faster performance wise, it has in-memory computation, mapreduce write to disk after each step.

7. narrow: each ouput partition have only one input(no shuffle)
wide: output partition depends on multiple input partitions and require shuffle.

6. lazy evaluation: spark does not execute the transformation unless we use action, it just build a logical plan
it reduce unnecessary computation and improve performance.

8. S3/ AWS Glue/ S3 or redshift

select max(salary) as SecondHighestSalary
from employee
where salary < (select max(salary) from employee);